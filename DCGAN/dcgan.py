# -*- coding: utf-8 -*-
"""DCGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wxOQAdVKF6IKnhe4K2duqIK0SEVK2bs8

# Deep Convolution GAN Frame
- Data
- Model
  - Generator
  - Discriminator
  - Loss
- Tool
  - FID
  - Visualize
  - GIF
- Train
  - Checkpoint
  - train_step()
  - train()
- Grading
  - Restore model/ckpt
  - Show image grid
  - Show mean FID

# Install Tensorflow & Mount Drive
"""

# Commented out IPython magic to ensure Python compatibility.
# Tensorboard
! pip install --upgrade tensorflow-gpu tensorboard

# To generate GIFs
!pip install -q imageio

# Load the TensorBoard notebook extension
# %load_ext tensorboard

import tensorflow as tf
print(tf.__version__)
print(str(tf.test.is_gpu_available()))

from google.colab import drive
drive.mount('/content/drive/')
# %cd /content/drive/'My Drive'/GAN/CSE676_Project1/DCGAN
!ls

"""# Import and Parameters"""

import os
import PIL
import glob
import time
import imageio
import datetime
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras import metrics
print(tf.__version__)

# For FID
from numpy import cov
from numpy import trace
from numpy import iscomplexobj
from numpy import asarray
from numpy.random import shuffle
from scipy.linalg import sqrtm
from skimage.transform import resize
from keras.datasets import cifar10

# For GIF
from IPython import display
from IPython.display import clear_output
from google.colab import files

begin = 0
epochs = 200
batch_size = 64
lamda = 10



inception3 = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))

"""# Data"""

def preprocess(x, y):
    x = tf.cast(x, tf.float32)
    x = x / 127.5 - 1
    y = tf.one_hot(y, depth=10, on_value=1.0, off_value=0.0, axis=-1) 
    return x, y

(x,y), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
print(x.shape, y.shape, x_test.shape, y_test.shape)

x = x.reshape(x.shape[0], 32, 32, 3).astype('float32')
x_test = x_test.reshape(x_test.shape[0], 32, 32, 3).astype('float32')

batch_train = 1024
batch_test = 1024

training = tf.data.Dataset.from_tensor_slices((x, y))
train_data_ = training.map(preprocess).shuffle(50000).batch(batch_train).repeat()
train_data_it = iter(train_data_)

testing = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_data_ = testing.map(preprocess).batch(batch_test).repeat()
test_data_it = iter(test_data_)

# training = tf.data.Dataset.from_tensor_slices((x, y))
train_data = training.map(preprocess).shuffle(50000).batch(batch_size)

# testing = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_data = testing.map(preprocess).batch(batch_size)


train_img = next(iter(train_data))
test_img = next(iter(test_data))
img, label = train_img
print(img.shape, label.shape)
print(img[0].shape, img[0].dtype, img[0].numpy().min(), img[0].numpy().max())
print(label[0].numpy())

fig = plt.figure(figsize=(8,8))
for i in range(64):
    plt.subplot(8, 8, i+1)
    plt.imshow(img[i] * 0.5 + 0.5 )
    plt.axis('off')
plt.savefig('dataset.png')

"""# Model

## Generator
"""

class Generator(keras.Model):

    def __init__(self):
        super(Generator, self).__init__()
        # input img is [b, 100]
        
        self.channel = 512
        self.dense = keras.layers.Dense(4*4*self.channel)  
        
        self.conv1 = keras.layers.Conv2DTranspose(256, 4, 2, 'same', use_bias=False)
        self.batch1 = keras.layers.BatchNormalization()
        
        self.conv2 = keras.layers.Conv2DTranspose(128, 4, 2, 'same', use_bias=False)
        self.batch2 = keras.layers.BatchNormalization()
        
        self.conv3 = keras.layers.Conv2DTranspose(64, 4, 2, 'same', use_bias=False)
        self.batch3 = keras.layers.BatchNormalization()
        
        self.conv4 = keras.layers.Conv2DTranspose(3, 3, 1, 'same', use_bias=False)
        
        
        return

    def call(self, inputs, training=None):
        # [b, 100] => [b, 4, 4, 512]
        x = self.dense(inputs)
        x = tf.reshape(x, shape = [-1, 4, 4, 512])
        x = tf.nn.leaky_relu(x)
        
        # [b, 4, 4, 512] => [b, 8, 8, 256]
        x = self.conv1(x)
        x = self.batch1(x, training = training)
        x = tf.nn.leaky_relu(x)
        
        # [b, 8, 8, 256] => [b, 16, 16, 128]
        x = self.conv2(x)
        x = self.batch2(x, training = training)
        x = tf.nn.leaky_relu(x)
        
        # [b, 16, 16, 128] => [b, 32, 32, 64]
        x = self.conv3(x)
        x = self.batch3(x, training = training)
        x = tf.nn.leaky_relu(x)
        
        # [b, 32, 32, 64] => [b, 32, 32, 3]
        x = self.conv4(x)
        x = tf.tanh(x)
        
        return x
    def model(self):
      
        x = keras.Input(shape=(100,)) 
        return keras.Model(inputs=[x], outputs=self.call(x))

"""## Discriminator"""

class Discriminator(keras.Model):

    def __init__(self):
        super(Discriminator, self).__init__()

        
        self.conv1 = keras.layers.Conv2D(64, 4, 2, 'same')

        self.conv2 = keras.layers.Conv2D(128, 4, 2, 'same')
        
        self.conv3 = keras.layers.Conv2D(256, 4, 2, 'same')
        
        self.conv4 = keras.layers.Conv2D(512, 3, 1, 'same')

        self.flatten = keras.layers.Flatten()

        self.dense = keras.layers.Dense(1)


        return

    def call(self, inputs, training=None):
        
        # [b, 32, 32, 3] => [b, 16, 16, 64]
        x = self.conv1(inputs)
        x = tf.nn.leaky_relu(x)
        
        # [b, 16, 16, 64] => [b, 8, 8, 128]
        x = self.conv2(x)
        x = tf.nn.leaky_relu(x)
        
        # [b, 8, 8, 128] => [b, 4, 4, 256]
        x = self.conv3(x)
        x = tf.nn.leaky_relu(x)
        
        # [b, 4, 4, 256] => [b, 4, 4, 512]
        x = self.conv4(x)
        x = tf.nn.leaky_relu(x)
        
        # [b, 4, 4, 512] => [b, 4*4*512]
        x = self.flatten(x)
        
        # [b, 2*2*256] => [b, 1]
        x = self.dense(x)
        
        return x
      
    def model(self):
      
        x = keras.Input(shape=(32,32,3,)) 
        return keras.Model(inputs=[x], outputs=self.call(x))

generator = Generator()
generator.build(input_shape=(1, 100))
generator.model().summary()
keras.utils.plot_model(generator.model(), 'dc_generator.png', show_shapes=True)
# noise = tf.random.normal([64, 100])
# generated_image = generator(noise, training=False)
# plt.imshow(generated_image[0, :, :, 2])
# print('geneated_img:', generated_image.shape, generated_image.dtype, tf.reduce_min(generated_image), tf.reduce_max(generated_image))

discriminator = Discriminator()
discriminator.build(input_shape=(1, 32, 32, 3))
discriminator.model().summary()
keras.utils.plot_model(discriminator.model(), 'dc_discriminator.png', show_shapes=True)
# decision = discriminator([generated_image, tf.random.normal([64, 10])])
# print ('decision:', decision.shape, tf.reduce_min(decision), tf.reduce_min(decision))

"""## Loss"""

loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)
def generator_loss(gen_output):
    
    loss = loss_obj(tf.ones_like(gen_output), gen_output)
    
    return tf.reduce_mean(loss)
  
def discriminator_loss(real_output, gen_output):
    
    loss1 = loss_obj(tf.ones_like(real_output), real_output)
    loss2 = loss_obj(tf.zeros_like(gen_output), gen_output)
    
    return tf.reduce_mean(loss1) + tf.reduce_mean(loss2)

"""# Tool

## FID 
referrence : https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/
"""

def scale_images(images, new_shape):
    images_list = list()
    for image in images:
        # resize with nearest neighbor interpolation
        new_image = resize(image, new_shape, 0)
        # store
        images_list.append(new_image)
        
    return asarray(images_list)

def calculate_fid(model, images1, images2):
	# calculate activations
	act1 = model.predict(images1)
	act2 = model.predict(images2)
	# calculate mean and covariance statistics
	mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)
	mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)
	# calculate sum squared difference between means
	ssdiff = np.sum((mu1 - mu2)**2.0)
	# calculate sqrt of product between cov
	covmean = sqrtm(sigma1.dot(sigma2))
	# check and correct imaginary numbers from sqrt
	if iscomplexobj(covmean):
		covmean = covmean.real
	# calculate score
	fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)
    
	return fid

def calculate_fid_with_img(img1, img2):
    img1 = scale_images(img1, (299,299,3))
    img2 = scale_images(img2, (299,299,3))
#     img1 = tf.keras.applications.inception_v3.preprocess_input(img1)
#     img2 = tf.keras.applications.inception_v3.preprocess_input(img2)
    # calcufate fid
    fid = calculate_fid(inception3, img1, img2)

    return fid

def calculate_all_fid(generator,  real_img, training):
    img, label = real_img
    seed = tf.random.normal([img.shape[0], 100])
    fake_img = generator(seed, training=training)
    fid = calculate_fid_with_img(img, fake_img)
    
    return fid

train_img = next(train_data_it)
test_img = next(test_data_it)
train_img, train_label = train_img
test_img, test_label = test_img

calculate_fid_with_img(train_img, test_img)

# import numpy
# from numpy import cov
# from numpy import trace
# from numpy import iscomplexobj
# from numpy import asarray
# from numpy.random import shuffle
# from scipy.linalg import sqrtm
# from keras.applications.inception_v3 import InceptionV3
# from keras.applications.inception_v3 import preprocess_input
# from keras.datasets.mnist import load_data
from skimage.transform import resize

# scale an array of images to a new size
def scale_images(images, new_shape):
    images_list = list()
    for image in images:
        # resize with nearest neighbor interpolation
        new_image = resize(image, new_shape, 0)
        # store
        images_list.append(new_image)
    return np.asarray(images_list)

# calculate frechet inception distance
def calculate_fid(model, images1, images2):
    # calculate activations
    act1 = model.predict(images1)
    act2 = model.predict(images2)
    # calculate mean and covariance statistics
    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)
    # calculate sum squared difference between means
    ssdiff = np.sum((mu1 - mu2)**2.0)
    # calculate sqrt of product between cov
    covmean = sqrtm(sigma1.dot(sigma2))
    # check and correct imaginary numbers from sqrt
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    # calculate score
    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
    return fid

# prepare the inception v3 model
model = tf.keras.applications.InceptionV3(include_top=False,
                                                weights='imagenet', pooling='avg')
# load cifar10 images
(images1, _), (images2, _) = tf.keras.datasets.cifar10.load_data()
np.random.shuffle(images1)
images1 = images1[:64]
images2 = images2[:64]
print('Loaded', images1.shape, images2.shape)
# convert integer to floating point values
images1 = images1.astype('float32')
images2 = images2.astype('float32')
# resize images
images1 = scale_images(images1, (299,299,3))
images2 = scale_images(images2, (299,299,3))
print('Scaled', images1.shape, images2.shape)
# pre-process images
images1 = tf.keras.applications.inception_v3.preprocess_input(images1)
images2 = tf.keras.applications.inception_v3.preprocess_input(images2)
# calculate fid
fid = calculate_fid(model, images1, images2)
print('FID: %.3f' % fid)

calculate_fid_with_img(images1, images2)

"""## For visualization"""

# def generate_and_save_images(model, epoch, seed, path, show=False):
#     predictions = model(seed, training=False)
#     fig = plt.figure(figsize=(8,8))

#     for i in range(64):
#         plt.subplot(8, 8, i+1)
#         plt.imshow(predictions[i] * 0.5 + 0.5 )
#         plt.axis('off')
#     plt.savefig(path + '{:04d}.png'.format(epoch))
#     if show == True:
#       plt.show()
#     plt.close()
    

def generate_and_save_images(model, epoch, seed, path, show=False):
    predictions = model(seed, training=False)
    fig = plt.figure(figsize=(8,8))

    for i in range(64):
        plt.subplot(8, 8, i+1)
        plt.imshow(predictions[i] * 0.5 + 0.5 )
        plt.axis('off')
    plt.savefig(path + '{:04d}.png'.format(epoch))
    if show == True:
      plt.show()
    plt.close()
    
    if show == True:
      fig = plt.figure(figsize=(8,8))
      for i in range(4):
        plt.subplot(2, 2, i+1)
        plt.imshow(predictions[i] * 0.5 + 0.5 )
        plt.axis('off')
      plt.savefig(path + '{:04d}_1.png'.format(epoch))
      plt.close()

def display_image(epoch_no):
    return PIL.Image.open('./train/train_epoch_{:04d}.png'.format(epoch_no))

"""## Make GIF

reference: https://www.tensorflow.org/tutorials/generative/dcgan
"""

def make_gif():
  anim_file = 'dcgan.gif'

  with imageio.get_writer(anim_file, mode='I') as writer:
      filenames = glob.glob("./train/train_epoch_*.png")
      filenames = sorted(filenames)
      last = -1
      for i,filename in enumerate(filenames):
          frame = 2*(i**0.5)
          if round(frame) > round(last):
              last = frame
          else:
              continue
          image = imageio.imread(filename)
          writer.append_data(image)
      image = imageio.imread(filename)
      writer.append_data(image)

  import IPython
  if IPython.version_info > (6,2,0,''):
      display.Image(filename=anim_file)

"""# Train

## Initialize model & optimizer
"""

Gen = Generator()
Dis = Discriminator()

g_optimizer = tf.keras.optimizers.Adam(1e-4)
d_optimizer = tf.keras.optimizers.Adam(1e-4)

"""## Checkpoint"""

checkpoint_dir = './ckpt/dc/'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(g_optimizer=g_optimizer, d_optimizer=d_optimizer, Gen=Gen, Dis=Dis)
print(checkpoint_prefix)

"""## Train_step"""

@tf.function
def train_step(real_img):
    img, label = real_img
    z = tf.random.normal([label.shape[0], 100])
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        gen_img = Gen(z, training=True)
        real_output = Dis(img, training=True)
        gene_output = Dis(gen_img, training=True)
        gen_loss = generator_loss(gene_output)
        disc_loss = discriminator_loss(real_output, gene_output)

    grad_gen = gen_tape.gradient(gen_loss, Gen.trainable_variables)
    grad_dis = disc_tape.gradient(disc_loss, Dis.trainable_variables)
    g_optimizer.apply_gradients(zip(grad_gen, Gen.trainable_variables))
    d_optimizer.apply_gradients(zip(grad_dis, Dis.trainable_variables))

    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss)

"""## Train()"""

def train(train_dataset, begin, epochs, iteration, batch_size):
    start_time = time.time()
    print('batch_size:', batch_size,', begin_iteration:', iteration, ', batch_train:', batch_train, ', batch_test:', batch_test, ', epochs:', epochs, ', gp lamda:', lamda)
    print('Begin time:{}.'.format( time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())))
    
    # seed is for generate_and_save_images, should not be change durnig train time
    seed = tf.random.normal([batch_size, 100])
    
    total_gen_loss = total_dis_loss = 0
    per_it = 1000
    for epoch in range(begin, epochs):
        for step, img in enumerate(train_dataset):
            iteration += 1
            gen_loss, disc_loss = train_step(img)
            total_gen_loss += float(gen_loss)
            total_dis_loss += float(disc_loss)
            if iteration % per_it == 0:
                test_it = next(test_data_it)
                img1024, lab1024 = test_it
                test_fid_score = calculate_all_fid(Gen, test_it, training=True)
                train_it = next(train_data_it)
                train_fid_score = calculate_all_fid(Gen, train_it, training=True)
                
                with train_summary_writer.as_default():
                    tf.summary.scalar('train_FID', train_fid_score, step=iteration)
                    tf.summary.scalar('test_FID', test_fid_score, step=iteration)
                    tf.summary.scalar('Generator', total_gen_loss / per_it, step=iteration)
                    tf.summary.scalar('Discriminator', total_dis_loss / per_it, step=iteration)
                print("Epoch: {}, iteration:{}, train_fid :{:.2f}, test_fid :{:.2f}, gene_loss: {:.2f}, disc_loss: {:.2f}.".format( epoch + 1, iteration, train_fid_score,test_fid_score,  total_gen_loss / per_it , total_dis_loss / per_it) )
                total_gen_loss = total_dis_loss = 0
                
        generate_and_save_images(Gen, epoch + 1, seed, './train/train_epoch_')

        if (epoch + 1) % 10 == 0:
            checkpoint.save(file_prefix = checkpoint_prefix)
            print('Epoch: {}, Current time:{}, save checkpoint successfully. '.format(epoch + 1, time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())))

    end_time = time.time()
    total_sec = end_time - start_time
    hours = int((total_sec)/(60*60))
    minitues = int((total_sec/60 - hours*60))
    seconds = int(total_sec - hours*3600 - minitues*60)
    print('End time:{}, total use time:{}hours, {}minitues, {}seconds.'.format(time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()), hours, minitues, seconds))

"""## Test()"""

from IPython.display import clear_output
def test(test_dataset, epochs, batch_size):
    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
    
    z = tf.random.normal([batch_size, 100])
    generate_and_save_images(Gen, epochs, z, './test/final_')
    
    fid_test = 0
    i = 0
    for step, img in enumerate(test_dataset):
      # 1280
        fid = calculate_all_fid(Gen, img, training=False)
        fid_test += fid
        i += 1
        if i >= 10:
            break
        print('{:04d} / {:04d} test imgs, FID score : {:.2f}, please wait... '.format(i*batch_size, 10*batch_size, fid))
    fid_test /= 10.
    print('The average FID score on test data is:', fid_test)

log_dir = './logs/' + 'dc'
train_summary_writer = tf.summary.create_file_writer(log_dir)

# Commented out IPython magic to ensure Python compatibility.
# !kill 379
# %tensorboard --logdir ./logs

"""## Main()"""

def main():
    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
    train(train_data, 0, epochs, 0, batch_size)
    make_gif()
    # use latest checkpoint to calculate it
    test(test_data, epochs, batch_test)

if __name__ == '__main__':
    main()

"""# For Grading

## Restore the best model
Please run this cell first to restore the best model
"""

import os
import PIL
import glob
import time
import imageio
import datetime
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras import metrics

# For FID
from numpy import cov
from numpy import trace
from numpy import iscomplexobj
from numpy import asarray
from numpy.random import shuffle
from scipy.linalg import sqrtm
from skimage.transform import resize
from keras.datasets import cifar10

batch_size = 64

inception3 = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))

def preprocess(x, y):
    x = tf.cast(x, tf.float32)
    x = x / 127.5 - 1
    y = tf.one_hot(y, depth=10, on_value=1.0, off_value=0.0, axis=-1) 
    return x, y

(x,y), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

x_test = x_test.reshape(x_test.shape[0], 32, 32, 3).astype('float32')

batch_test = 1024

testing = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_data_ = testing.map(preprocess).batch(batch_test).repeat()
test_data_it = iter(test_data_)

# testing = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_data = testing.map(preprocess).batch(batch_size)
test_img = next(iter(test_data))


class Generator(keras.Model):

    def __init__(self):
        super(Generator, self).__init__()
        # input img is [b, 100]
        
        self.channel = 512
        self.dense = keras.layers.Dense(4*4*self.channel)  
        
        self.conv1 = keras.layers.Conv2DTranspose(256, 4, 2, 'same', use_bias=False)
        self.batch1 = keras.layers.BatchNormalization()
        
        self.conv2 = keras.layers.Conv2DTranspose(128, 4, 2, 'same', use_bias=False)
        self.batch2 = keras.layers.BatchNormalization()
        
        self.conv3 = keras.layers.Conv2DTranspose(64, 4, 2, 'same', use_bias=False)
        self.batch3 = keras.layers.BatchNormalization()
        
        self.conv4 = keras.layers.Conv2DTranspose(3, 3, 1, 'same', use_bias=False)
        
        
        return

    def call(self, inputs, training=None):
        # [b, 100] => [b, 4, 4, 512]
        x = self.dense(inputs)
        x = tf.reshape(x, shape = [-1, 4, 4, 512])
        x = tf.nn.leaky_relu(x)
        
        # [b, 4, 4, 512] => [b, 8, 8, 256]
        x = self.conv1(x)
        x = self.batch1(x, training = training)
        x = tf.nn.leaky_relu(x)
        
        # [b, 8, 8, 256] => [b, 16, 16, 128]
        x = self.conv2(x)
        x = self.batch2(x, training = training)
        x = tf.nn.leaky_relu(x)
        
        # [b, 16, 16, 128] => [b, 32, 32, 64]
        x = self.conv3(x)
        x = self.batch3(x, training = training)
        x = tf.nn.leaky_relu(x)
        
        # [b, 32, 32, 64] => [b, 32, 32, 3]
        x = self.conv4(x)
        x = tf.tanh(x)
        
        return x

class Discriminator(keras.Model):

    def __init__(self):
        super(Discriminator, self).__init__()

        
        self.conv1 = keras.layers.Conv2D(64, 4, 2, 'same')

        self.conv2 = keras.layers.Conv2D(128, 4, 2, 'same')
        
        self.conv3 = keras.layers.Conv2D(256, 4, 2, 'same')
        
        self.conv4 = keras.layers.Conv2D(512, 3, 1, 'same')

        self.flatten = keras.layers.Flatten()

        self.dense = keras.layers.Dense(1)


        return

    def call(self, inputs, training=None):
        
        # [b, 32, 32, 3] => [b, 16, 16, 64]
        x = self.conv1(inputs)
        x = tf.nn.leaky_relu(x)
        
        # [b, 16, 16, 64] => [b, 8, 8, 128]
        x = self.conv2(x)
        x = tf.nn.leaky_relu(x)
        
        # [b, 8, 8, 128] => [b, 4, 4, 256]
        x = self.conv3(x)
        x = tf.nn.leaky_relu(x)
        
        # [b, 4, 4, 256] => [b, 4, 4, 512]
        x = self.conv4(x)
        x = tf.nn.leaky_relu(x)
        
        # [b, 4, 4, 512] => [b, 4*4*512]
        x = self.flatten(x)
        
        # [b, 2*2*256] => [b, 1]
        x = self.dense(x)
        
        return x

def scale_images(images, new_shape):
    images_list = list()
    for image in images:
        # resize with nearest neighbor interpolation
        new_image = resize(image, new_shape, 0)
        # store
        images_list.append(new_image)
        
    return asarray(images_list)

def calculate_fid(model, images1, images2):
    # calculate activations
    act1 = model.predict(images1)
    act2 = model.predict(images2)
    # calculate mean and covariance statistics
    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)
    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)
    # calculate sum squared difference between means
    ssdiff = np.sum((mu1 - mu2)**2.0)
    # calculate sqrt of product between cov
    covmean = sqrtm(sigma1.dot(sigma2))
    # check and correct imaginary numbers from sqrt
    if iscomplexobj(covmean):
        covmean = covmean.real
    # calculate score
    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)
    
    return fid

def calculate_fid_with_img(img1, img2):
    img1 = scale_images(img1, (299,299,3))
    img2 = scale_images(img2, (299,299,3))
    img1 = tf.keras.applications.inception_v3.preprocess_input(img1)
    img2 = tf.keras.applications.inception_v3.preprocess_input(img2)
    # calcufate fid
    fid = calculate_fid(inception3, img1, img2)

    return fid

def calculate_all_fid(generator,  real_img, training):
    img, label = real_img
    seed = tf.random.normal([img.shape[0], 100])
    fake_img = generator(seed, training=training)
    fid = calculate_fid_with_img(img, fake_img)
    
    return fid  

def generate_and_save_images(model, epoch, seed, path, show=False):
    predictions = model(seed, training=False)
    fig = plt.figure(figsize=(8,8))

    for i in range(64):
        plt.subplot(8, 8, i+1)
        plt.imshow(predictions[i] * 0.5 + 0.5 )
        plt.axis('off')
    plt.savefig(path + '{:04d}.png'.format(epoch))
    if show == True:
      plt.show()
    plt.close()
    
    if show == True:
      fig = plt.figure(figsize=(8,8))
      for i in range(4):
        plt.subplot(2, 2, i+1)
        plt.imshow(predictions[i] * 0.5 + 0.5 )
        plt.axis('off')
      plt.savefig(path + '{:04d}_1.png'.format(epoch))
      plt.close()

# restore the best checkpoint
# from dc_model import *

Gen = Generator()
Dis = Discriminator()
g_optimizer = tf.keras.optimizers.Adam(1e-4)
d_optimizer = tf.keras.optimizers.Adam(1e-4)

checkpoint_dir = './ckpt/dc/'
checkpoint_best_prefix = os.path.join(checkpoint_dir, "ckpt-8")
checkpoint = tf.train.Checkpoint(g_optimizer=g_optimizer, d_optimizer=d_optimizer, Gen=Gen, Dis=Dis)
checkpoint.restore(checkpoint_best_prefix)

"""## Show Image grid
This cell will use random noise to create a 8*8 image grid
"""

best = 8
z = tf.random.normal([batch_size, 100])
generate_and_save_images(Gen, best*10, z, './final_', show=True)

"""## Show Mean FID
This cell will calculate the mean FID over 10240 TEST data

notice: 240/10240 is duplicated
"""

fid_test = 0
i = 0
for step, img in enumerate(test_data):
  # 1280
    fid = calculate_all_fid(Gen, img, training=False)
    fid_test += fid
    i += 1
    print('{:04d} / {:04d} test imgs, FID score : {:.2f}, please wait... '.format(i*batch_test, 10*batch_test, fid))
    if i >= 10:
        break
fid_test /= 10.
print("The average FID score on total {} test img is :  {:.2f}.".format(10*batch_test, fid_test))